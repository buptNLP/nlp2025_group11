#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ChatGLMå…¼å®¹æ€§è¡¥ä¸æ¨¡å—
ä¸“é—¨ç”¨äºä¿®å¤ChatGLMTokenizerçš„padding_sideå‚æ•°é—®é¢˜
"""

import logging
import types

logger = logging.getLogger(__name__)

def apply_chatglm_tokenizer_patch():
    """åº”ç”¨ChatGLM tokenizerçš„å…¼å®¹æ€§è¡¥ä¸"""
    
    try:
        logger.info("ğŸ”§ å¼€å§‹åº”ç”¨ChatGLMå…¨é¢å…¼å®¹æ€§è¡¥ä¸...")
        
        # 1. é¦–å…ˆä¿®å¤PreTrainedTokenizerBase
        from transformers.tokenization_utils_base import PreTrainedTokenizerBase
        
        if not hasattr(PreTrainedTokenizerBase, '_original_pad_method'):
            PreTrainedTokenizerBase._original_pad_method = PreTrainedTokenizerBase._pad
            
            def safe_pad(self, encoded_inputs, *args, **kwargs):
                # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                kwargs.pop('padding_side', None)
                kwargs.pop('pad_to_multiple_of', None)
                return PreTrainedTokenizerBase._original_pad_method(self, encoded_inputs, *args, **kwargs)
            
            PreTrainedTokenizerBase._pad = safe_pad
            logger.info("âœ… PreTrainedTokenizerBase._pad è¡¥ä¸åº”ç”¨æˆåŠŸ")
        
        # 2. ä¿®å¤AutoTokenizerçš„from_pretrainedæ–¹æ³•
        from transformers import AutoTokenizer
        
        if not hasattr(AutoTokenizer, '_original_from_pretrained'):
            AutoTokenizer._original_from_pretrained = AutoTokenizer.from_pretrained
            
            @classmethod
            def patched_from_pretrained(cls, *args, **kwargs):
                tokenizer = AutoTokenizer._original_from_pretrained(*args, **kwargs)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ChatGLMTokenizerå¹¶åº”ç”¨è¡¥ä¸
                if hasattr(tokenizer, '_pad') and 'ChatGLM' in tokenizer.__class__.__name__:
                    logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ChatGLMTokenizer: {tokenizer.__class__.__name__}")
                    
                    # ä¿å­˜åŸå§‹æ–¹æ³•
                    original_pad = tokenizer._pad
                    
                    def safe_tokenizer_pad(encoded_inputs, *args, **kwargs):
                        # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                        kwargs.pop('padding_side', None)
                        kwargs.pop('pad_to_multiple_of', None)
                        return original_pad(encoded_inputs, *args, **kwargs)
                    
                    # ç»‘å®šæ–°æ–¹æ³•åˆ°å®ä¾‹
                    tokenizer._pad = types.MethodType(lambda self, *args, **kwargs: safe_tokenizer_pad(*args, **kwargs), tokenizer)
                    logger.info("âœ… ChatGLMTokenizerå®ä¾‹è¡¥ä¸åº”ç”¨æˆåŠŸ")
                
                return tokenizer
            
            AutoTokenizer.from_pretrained = patched_from_pretrained
            logger.info("âœ… AutoTokenizer.from_pretrained è¡¥ä¸åº”ç”¨æˆåŠŸ")
        
        # 3. ä¿®å¤ChatGLMæ¨¡å‹çš„_extract_past_from_model_outputæ–¹æ³•
        apply_chatglm_model_patch()
        
        logger.info("ğŸ‰ æ‰€æœ‰ChatGLMå…¼å®¹æ€§è¡¥ä¸åº”ç”¨å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ChatGLMå…¼å®¹æ€§è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def apply_chatglm_model_patch():
    """åº”ç”¨ChatGLMæ¨¡å‹çš„å…¼å®¹æ€§è¡¥ä¸"""
    try:
        logger.info("ğŸ”§ åº”ç”¨ChatGLMæ¨¡å‹å…¼å®¹æ€§è¡¥ä¸...")
        from transformers import AutoModelForCausalLM
        
        # ä¿®å¤AutoModelForCausalLMçš„from_pretrainedæ–¹æ³•
        if not hasattr(AutoModelForCausalLM, '_original_model_from_pretrained'):
            AutoModelForCausalLM._original_model_from_pretrained = AutoModelForCausalLM.from_pretrained
            
            @classmethod
            def patched_model_from_pretrained(cls, *args, **kwargs):
                model = AutoModelForCausalLM._original_model_from_pretrained(*args, **kwargs)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ChatGLMæ¨¡å‹å¹¶åº”ç”¨è¡¥ä¸
                if 'ChatGLM' in model.__class__.__name__:
                    logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ChatGLMæ¨¡å‹: {model.__class__.__name__}")
                    
                    # æ·»åŠ ç¼ºå¤±çš„_extract_past_from_model_outputæ–¹æ³•
                    if not hasattr(model, '_extract_past_from_model_output'):
                        def _extract_past_from_model_output(self, outputs, standardize_cache_format=False):
                            """ä»æ¨¡å‹è¾“å‡ºä¸­æå–past_key_values"""
                            if hasattr(outputs, 'past_key_values'):
                                return outputs.past_key_values
                            elif isinstance(outputs, tuple) and len(outputs) > 1:
                                # å‡è®¾past_key_valuesæ˜¯ç¬¬äºŒä¸ªå…ƒç´ 
                                return outputs[1] if outputs[1] is not None else None
                            else:
                                return None
                        
                        # ç»‘å®šæ–¹æ³•åˆ°æ¨¡å‹å®ä¾‹
                        model._extract_past_from_model_output = types.MethodType(_extract_past_from_model_output, model)
                        logger.info("âœ… ChatGLMæ¨¡å‹_extract_past_from_model_outputæ–¹æ³•å·²æ·»åŠ ")
                
                return model
            
            AutoModelForCausalLM.from_pretrained = patched_model_from_pretrained
            logger.info("âœ… AutoModelForCausalLM.from_pretrained è¡¥ä¸åº”ç”¨æˆåŠŸ")
        
        logger.info("âœ… ChatGLMæ¨¡å‹å…¼å®¹æ€§è¡¥ä¸åº”ç”¨å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ChatGLMæ¨¡å‹è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def patch_existing_tokenizer(tokenizer):
    """ä¸ºå·²åŠ è½½çš„tokenizeråº”ç”¨è¡¥ä¸"""
    try:
        if hasattr(tokenizer, '_pad') and 'ChatGLM' in tokenizer.__class__.__name__:
            logger.info(f"ğŸ¯ ä¸ºç°æœ‰tokenizeråº”ç”¨è¡¥ä¸: {tokenizer.__class__.__name__}")
            
            # ä¿å­˜åŸå§‹æ–¹æ³•
            original_pad = tokenizer._pad
            
            def safe_existing_pad(encoded_inputs, *args, **kwargs):
                # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                kwargs.pop('padding_side', None)
                kwargs.pop('pad_to_multiple_of', None)
                return original_pad(encoded_inputs, *args, **kwargs)
            
            # ç»‘å®šæ–°æ–¹æ³•
            tokenizer._pad = types.MethodType(lambda self, *args, **kwargs: safe_existing_pad(*args, **kwargs), tokenizer)
            logger.info("âœ… ç°æœ‰tokenizerè¡¥ä¸åº”ç”¨æˆåŠŸ")
            return True
    except Exception as e:
        logger.error(f"âŒ ç°æœ‰tokenizerè¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
    
    return False

def patch_existing_model(model):
    """ä¸ºå·²åŠ è½½çš„æ¨¡å‹åº”ç”¨è¡¥ä¸"""
    try:
        if 'ChatGLM' in model.__class__.__name__:
            logger.info(f"ğŸ¯ ä¸ºç°æœ‰æ¨¡å‹åº”ç”¨è¡¥ä¸: {model.__class__.__name__}")
            
            # æ·»åŠ ç¼ºå¤±çš„_extract_past_from_model_outputæ–¹æ³•
            if not hasattr(model, '_extract_past_from_model_output'):
                def _extract_past_from_model_output(self, outputs, standardize_cache_format=False):
                    """ä»æ¨¡å‹è¾“å‡ºä¸­æå–past_key_values"""
                    if hasattr(outputs, 'past_key_values'):
                        return outputs.past_key_values
                    elif isinstance(outputs, tuple) and len(outputs) > 1:
                        # å‡è®¾past_key_valuesæ˜¯ç¬¬äºŒä¸ªå…ƒç´ 
                        return outputs[1] if outputs[1] is not None else None
                    else:
                        return None
                
                # ç»‘å®šæ–¹æ³•åˆ°æ¨¡å‹å®ä¾‹
                model._extract_past_from_model_output = types.MethodType(_extract_past_from_model_output, model)
                logger.info("âœ… ç°æœ‰æ¨¡å‹_extract_past_from_model_outputæ–¹æ³•å·²æ·»åŠ ")
                return True
    except Exception as e:
        logger.error(f"âŒ ç°æœ‰æ¨¡å‹è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
    
    return False

# è‡ªåŠ¨åº”ç”¨è¡¥ä¸ï¼ˆå½“æ¨¡å—è¢«å¯¼å…¥æ—¶ï¼‰
if __name__ != "__main__":
    apply_chatglm_tokenizer_patch() 