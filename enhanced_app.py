import os
import gc
import json
import torch
import pickle
import logging
import re
from datetime import datetime
from flask import Flask, render_template, request, jsonify, session
from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification
import uuid
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ChatGLMå…¼å®¹æ€§è¡¥ä¸
try:
    from chatglm_patch import apply_chatglm_tokenizer_patch, patch_existing_tokenizer, patch_existing_model
    logger = logging.getLogger(__name__)
    logger.info("âœ… ChatGLMå…¼å®¹æ€§è¡¥ä¸æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ ChatGLMå…¼å®¹æ€§è¡¥ä¸æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    apply_chatglm_tokenizer_patch = None
    patch_existing_tokenizer = None
    patch_existing_model = None

# å¯¼å…¥ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨
from traditional_bias_detector import TraditionalBiasDetector

# å»¶è¿Ÿå¯¼å…¥å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
from enhanced_bias_correction_system import EnhancedBiasCorrectionSystem

# ä¼ ç»Ÿåè§æ£€æµ‹å™¨é€‚é…å™¨ç±»
class TraditionalBiasDetectorAdapter:
    """ä¼ ç»Ÿåè§æ£€æµ‹å™¨é€‚é…å™¨ - å°†ä¼˜åŒ–åçš„ä¼ ç»Ÿæ£€æµ‹å™¨é€‚é…ä¸ºWebåº”ç”¨æ¥å£"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼ ç»Ÿåè§æ£€æµ‹å™¨é€‚é…å™¨"""
        self.detector = TraditionalBiasDetector()
        
        # åè§ç±»å‹æ˜ å°„
        self.bias_type_mapping = {
            'gender': 'æ€§åˆ«åè§',
            'race': 'ç§æ—åè§', 
            'region': 'åœ°åŸŸåè§',
            'age': 'å¹´é¾„åè§',
            'occupation': 'èŒä¸šåè§'
        }
        
        # é£é™©ç­‰çº§æ˜ å°„
        self.risk_level_mapping = {
            'severe': 'very_high',
            'high': 'high',
            'medium': 'medium', 
            'low': 'low',
            'minimal': 'low'
        }
    
    def detect_bias(self, text):
        """æ£€æµ‹æ–‡æœ¬åè§å¹¶è¿”å›æ ‡å‡†æ ¼å¼ç»“æœ"""
        try:
            # ä½¿ç”¨ä¼˜åŒ–åçš„ä¼ ç»Ÿæ£€æµ‹å™¨
            result = self.detector.detect_bias(text, threshold_svm=0.3, threshold_sentiment=0.2)
            
            if not result:
                return None
            
            summary = result.get('summary', {})
            detection_results = result.get('detection_results', {})
            
            # åŸºæœ¬ä¿¡æ¯
            is_biased = summary.get('is_biased', False)
            confidence = summary.get('confidence', 0.0)
            bias_types = summary.get('bias_types', [])
            
            # è·å–è¯¦ç»†æ£€æµ‹ä¿¡æ¯
            sensitive_words = detection_results.get('sensitive_words', {})
            svm_prediction = detection_results.get('svm_prediction', {})
            sentiment_analysis = detection_results.get('sentiment_analysis', {})
            fairness_check = detection_results.get('fairness_check', {})
            
            # ç¡®å®šé£é™©ç­‰çº§
            fairness_severity = fairness_check.get('max_severity', 0)
            if is_biased:
                if fairness_severity >= 0.9:
                    risk_level = 'very_high'
                elif fairness_severity >= 0.8:
                    risk_level = 'high'
                elif fairness_severity >= 0.6:
                    risk_level = 'medium'
                else:
                    risk_level = 'low'
            else:
                risk_level = 'low'
            
            # æ„å»ºåè§ç±»å‹æè¿°
            detected_bias_types = []
            for bias_type in bias_types:
                if bias_type in self.bias_type_mapping:
                    detected_bias_types.append(bias_type)
            
            # ç”Ÿæˆæ‘˜è¦
            if is_biased:
                summary_text = f"æ£€æµ‹åˆ°åè§å†…å®¹ (ç½®ä¿¡åº¦: {confidence:.1%})"
                if detected_bias_types:
                    types_str = 'ã€'.join([self.bias_type_mapping.get(t, t) for t in detected_bias_types])
                    summary_text += f"ï¼Œæ¶‰åŠ: {types_str}"
                
                # æ·»åŠ å…·ä½“è¿è§„ä¿¡æ¯
                violations = fairness_check.get('violations', [])
                if violations:
                    main_violation = violations[0]
                    violation_desc = main_violation.get('description', 'æœªçŸ¥è¿è§„')
                    summary_text += f"\nä¸»è¦è¿è§„: {violation_desc}"
            else:
                summary_text = f"å†…å®¹å®‰å…¨ï¼Œæœªæ£€æµ‹åˆ°åè§ (ç½®ä¿¡åº¦: {confidence:.1%})"
            
            # æ„å»ºæ ‡å‡†æ ¼å¼ç»“æœ
            adapted_result = {
                'method': 'traditional',
                'overall_bias': is_biased,
                'overall_confidence': confidence,
                'overall_risk_level': risk_level,
                'detected_bias_types': detected_bias_types,
                'summary': summary_text,
                
                # è¯¦ç»†æ£€æµ‹ä¿¡æ¯
                'details': {
                    'sensitive_words': sensitive_words,
                    'svm_probability': svm_prediction.get('probability', 0),
                    'sentiment_intensity': sentiment_analysis.get('intensity', 0),
                    'fairness_severity': fairness_severity,
                    'fairness_violations': len(fairness_check.get('violations', [])),
                    'detection_flow': detection_results.get('flow', [])
                },
                
                # åŸå§‹ç»“æœï¼ˆè°ƒè¯•ç”¨ï¼‰
                'raw_result': result
            }
            
            return adapted_result
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿåè§æ£€æµ‹å™¨é€‚é…å™¨å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def get_detector_info(self):
        """è·å–æ£€æµ‹å™¨ä¿¡æ¯"""
        return {
            'name': 'ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨',
            'version': '2.0',
            'description': 'åŸºäºCOLDatasetå¢å¼ºçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨ï¼Œæ”¯æŒå¹¶è¡Œæ£€æµ‹ç­–ç•¥',
            'features': [
                'COLDatasetè®­ç»ƒçš„æ•æ„Ÿè¯è¯å…¸',
                'COLDatasetè®­ç»ƒçš„SVMåˆ†ç±»å™¨',
                'COLDatasetå¢å¼ºçš„æƒ…æ„Ÿåˆ†æå™¨',
                'æ™ºèƒ½å…¬å¹³æ€§è§„åˆ™æ£€æŸ¥',
                'å¹¶è¡Œæ£€æµ‹ç­–ç•¥',
                'ä¸¥é‡è¿è§„ç›´åˆ¤'
            ],
            'supported_bias_types': list(self.bias_type_mapping.keys()),
            'detection_components': [
                'æ•æ„Ÿè¯æ­£åˆ™åŒ¹é…',
                'SVMæœºå™¨å­¦ä¹ åˆ†ç±»',
                'æƒ…æ„Ÿå€¾å‘åˆ†æ', 
                'å…¬å¹³æ€§è§„åˆ™æ£€æŸ¥'
            ]
        }

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # å¼ºåˆ¶è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('enhanced_app.log', encoding='utf-8')  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.secret_key = 'your-secret-key-here'

# è®¾ç½®ç»Ÿä¸€æœ¬åœ°æ¨¡å‹å­˜å‚¨ç›®å½•
BASE_DIR = "D:/models"

# æ˜¯å¦å¼€å¯åè§æµ‹è¯•æ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰
BIAS_TEST_MODE = False

# åè§æ£€æµ‹æ–¹æ³•é…ç½®
BIAS_DETECTION_METHODS = {
    'bert': {
        'name': 'åŸºäºBertçš„æ·±åº¦å­¦ä¹ ',
        'description': 'ä½¿ç”¨COLDatasetæ•°æ®é›†è®­ç»ƒçš„BERTæ¨¡å‹è¿›è¡Œåè§æ£€æµ‹',
        'enabled': True,
        'icon': 'fa-brain',
        'color': '#7c3aed'
    },
    'traditional': {
        'name': 'ä¼˜åŒ–åçš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ ',
        'description': 'åŸºäºCOLDatasetå¢å¼ºçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨ï¼Œé›†æˆSVMåˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œæ™ºèƒ½å…¬å¹³æ€§è§„åˆ™',
        'enabled': True,
        'icon': 'fa-cog',
        'color': '#059669'
    }
}

# é»˜è®¤åè§æ£€æµ‹æ–¹æ³•
DEFAULT_BIAS_METHOD = 'bert'



# BERTåè§æ£€æµ‹å™¨ç±»ï¼ˆä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰
class BertBiasDetector:
    """BERTä¸­æ–‡åè§æ£€æµ‹å™¨ - åŸºäºCOLDatasetæ•°æ®é›†è®­ç»ƒçš„æ–°æ¨¡å‹"""
    
    def __init__(self, model_path='./coldataset_bias_bert_model'):
        """åˆå§‹åŒ–BERTåè§æ£€æµ‹å™¨"""
        self.model_path = model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # COLDatasetæ¨¡å‹çš„äºŒåˆ†ç±»æ ‡ç­¾æ˜ å°„ï¼ˆ0=safeï¼Œ1=offensiveï¼‰
        self.label_map = {
            'safe': 0,          # å®‰å…¨å†…å®¹
            'offensive': 1      # æ”»å‡»æ€§å†…å®¹
        }
        
        self.id2label = {v: k for k, v in self.label_map.items()}
        
        # ä¸­æ–‡æ ‡ç­¾æè¿°
        self.label_descriptions = {
            'safe': 'å®‰å…¨å†…å®¹',
            'offensive': 'æ”»å‡»æ€§å†…å®¹'
        }
        
        # åè§ç±»å‹å…³é”®è¯ï¼ˆåŸºäºCOLDatasetæ•°æ®é›†çš„ç±»å‹ï¼šç§æ—ã€æ€§åˆ«ã€åœ°åŸŸï¼‰
        self.bias_keywords = {
            'race': ['ç§æ—', 'æ°‘æ—', 'æ±‰æ—', 'ç»´æ—', 'å›æ—', 'è—æ—', 'è’™å¤', 'æ»¡æ—', 'æœé²œæ—'],
            'gender': ['ç”·', 'å¥³', 'æ€§åˆ«', 'ç”·æ€§', 'å¥³æ€§', 'ç”·äºº', 'å¥³äºº', 'å…ˆç”Ÿ', 'å¥³å£«', 'å°å§', 'å¦‡å¥³'],
            'region': ['åœ°åŸŸ', 'åŸå¸‚', 'å†œæ‘', 'åŒ—æ–¹', 'å—æ–¹', 'ä¸œåŒ—', 'è¥¿éƒ¨', 'ä¸Šæµ·', 'åŒ—äº¬', 'å¤–åœ°', 'ä¹¡ä¸‹', 'å±±åŒº']
        }
        
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½COLDatasetè®­ç»ƒçš„BERTåè§æ£€æµ‹æ¨¡å‹"""
        try:
            logger.info(f"ğŸ¤– åŠ è½½COLDatasetè®­ç»ƒçš„BERTåè§æ£€æµ‹æ¨¡å‹: {self.model_path}")
            
            if not os.path.exists(self.model_path):
                logger.error(f"COLDataset BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œcoldataset_bias_trainer.pyè®­ç»ƒæ¨¡å‹")
                return False
            
            # ä½¿ç”¨AutoTokenizerå’ŒAutoModelForSequenceClassification
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            
            # ç›´æ¥ä»æ¨¡å‹ç›®å½•åŠ è½½tokenizerå’Œmodel
            logger.info("æ­£åœ¨åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            
            logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… COLDataset BERTåè§æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ!")
            logger.info(f"   æ¨¡å‹å‚æ•°é‡: {self.model.num_parameters():,}")
            logger.info(f"   è®¾å¤‡: {self.device}")
            logger.info(f"   æ ‡ç­¾æ˜ å°„: {self.label_descriptions}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ COLDataset BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.error("ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def detect_bias(self, text):
        """ä½¿ç”¨COLDatasetè®­ç»ƒçš„BERTæ¨¡å‹æ£€æµ‹åè§"""
        if not self.model or not self.tokenizer:
            logger.warning("COLDataset BERTæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ£€æµ‹åè§")
            return None
        
        try:
            # æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯ï¼ˆä½¿ç”¨512æœ€å¤§é•¿åº¦ä»¥é€‚åº”COLDatasetæ¨¡å‹ï¼‰
            inputs = self.tokenizer(
                text,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
                
                predicted_id = torch.argmax(logits, dim=-1).item()
                predicted_label = self.id2label[predicted_id]
                
                # è·å–ä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡
                prob_safe = probabilities[0][0].item()
                prob_offensive = probabilities[0][1].item()
                confidence = max(prob_safe, prob_offensive)
            
            # åˆ†æåè§ç±»å‹
            detected_bias_types = []
            text_lower = text.lower()
            for bias_type, keywords in self.bias_keywords.items():
                for kw in keywords:
                    # ä¼˜å…ˆå…¨è¯åŒ¹é…
                    if kw in text:
                        detected_bias_types.append(bias_type)
                        break
                    # å…¶æ¬¡å°è¯•ç©ºæ ¼/æ ‡ç‚¹åˆ†éš”çš„å­ä¸²åŒ¹é…
                    if f' {kw} ' in text_lower or f'{kw}ï¼Œ' in text_lower or f'{kw}ã€‚' in text_lower:
                        detected_bias_types.append(bias_type)
                        break
            # å»é‡ï¼Œä¼˜å…ˆæ€§åˆ«>åœ°åŸŸ>ç§æ—
            priority = ['gender', 'region', 'race']
            detected_bias_types = sorted(set(detected_bias_types), key=lambda x: priority.index(x) if x in priority else 99)
            
            # æ„å»ºç»“æœ
            is_biased = predicted_label == 'offensive'
            
            # è·å–é£é™©ç­‰çº§ï¼ˆåŸºäºCOLDatasetæ¨¡å‹çš„ç½®ä¿¡åº¦ï¼‰
            if is_biased:
                if confidence >= 0.9:
                    risk_level = 'very_high'
                elif confidence >= 0.8:
                    risk_level = 'high'
                elif confidence >= 0.7:
                    risk_level = 'medium'
                else:
                    risk_level = 'low'
            else:
                risk_level = 'low'
            
            # ç”Ÿæˆæ‘˜è¦
            if is_biased:
                summary = f"æ£€æµ‹åˆ°{self.label_descriptions[predicted_label]} (ç½®ä¿¡åº¦: {confidence:.1%})"
                if detected_bias_types:
                    bias_type_names = {
                        'race': 'ç§æ—åè§',
                        'gender': 'æ€§åˆ«åè§',
                        'region': 'åœ°åŸŸåè§'
                    }
                    types_str = 'ã€'.join([bias_type_names.get(t, t) for t in detected_bias_types])
                    summary += f"ï¼Œå¯èƒ½æ¶‰åŠ: {types_str}"
            else:
                summary = f"å†…å®¹å®‰å…¨ï¼Œæœªæ£€æµ‹åˆ°æ”»å‡»æ€§å†…å®¹ (ç½®ä¿¡åº¦: {confidence:.1%})"
            
            result = {
                'method': 'bert',
                'overall_bias': is_biased,
                'overall_confidence': confidence,
                'overall_risk_level': risk_level,
                'predicted_label': predicted_label,
                'predicted_label_zh': self.label_descriptions[predicted_label],
                'detected_bias_types': detected_bias_types,
                'summary': summary,
                'model_version': 'BERT-base-chinese + COLDataset',
                'model_performance': {
                    'offensive_recall': '87.2%',
                    'offensive_f1': '79.0%',
                    'accuracy': '81.6%',
                    'training_dataset': 'COLDataset'
                },
                'probability_distribution': {
                    'safe': prob_safe,
                    'offensive': prob_offensive
                },
                'categories': {
                    predicted_label: {
                        'confidence': confidence,
                        'risk_level': risk_level
                    }
                }
            }
            
            logger.info(f"ğŸ” COLDataset BERTåè§æ£€æµ‹å®Œæˆ: {predicted_label} ({confidence:.1%})")
            return result
            
        except Exception as e:
            logger.error(f"COLDataset BERTåè§æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None



# æ¨¡å‹ä¿¡æ¯é…ç½®
model_info = {
    "chatglm3": {
        "repo": "THUDM/chatglm3-6b",
        "path": "C:/Users/AWLENWARE/.cache/modelscope/hub/ZhipuAI/chatglm3-6b",
        "use_cpu": True,
        "display_name": "ChatGLM3-6B",
        "description": "æ™ºè°±AIå¼€å‘çš„å¯¹è¯æ¨¡å‹",
        "bias_test_params": {
            "temperature": 0.9,
            "max_tokens": 350,
            "top_p": 0.95
        }
    },
    "yi6b": {
        "repo": "01-ai/Yi-6B-Chat",
        "path": "D:/models/yi6b/models--01-ai--Yi-6B-Chat/snapshots/2dbf63b0cb7bc493c0243502c6e6111a36e3a093",
        "display_name": "Yi-6B-Chat",
        "description": "é›¶ä¸€ä¸‡ç‰©å¼€å‘çš„å¯¹è¯æ¨¡å‹",
        "bias_test_params": {
            "temperature": 0.9,
            "max_tokens": 350,
            "top_p": 0.9
        }
    },
    "qwen7b": {
        "repo": "Qwen/Qwen-7B-Chat",
        "path": "D:/models/qwen7b/models--Qwen--Qwen-7B-Chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83",
        "low_memory": True,
        "display_name": "Qwen-7B-Chat",
        "description": "é˜¿é‡Œäº‘å¼€å‘çš„åƒé—®æ¨¡å‹",
        "bias_test_params": {
            "temperature": 0.95,
            "max_tokens": 350,
            "top_p": 0.95
        }
    }
}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œæ£€æµ‹å™¨
loaded_models = {}
traditional_bias_detector = None  # ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨
bert_bias_detector = None  # BERTæ·±åº¦å­¦ä¹ æ£€æµ‹å™¨
bias_corrector = None  # åè§çº æ­£å™¨

def init_bias_detectors():
    """åˆå§‹åŒ–åè§æ£€æµ‹å™¨"""
    global traditional_bias_detector, bert_bias_detector
    
    # åˆå§‹åŒ–COLDataset BERTåè§æ£€æµ‹å™¨ï¼ˆä¼˜å…ˆï¼‰
    try:
        bert_bias_detector = BertBiasDetector()
        logger.info("âœ… COLDataset BERTæ·±åº¦å­¦ä¹ åè§æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ¯ æ¨¡å‹æ€§èƒ½: æ”»å‡»æ€§å†…å®¹å¬å›ç‡87.2%, F1åˆ†æ•°79.0%, æ€»ä½“å‡†ç¡®ç‡81.6%")
    except Exception as e:
        logger.error(f"âŒ COLDataset BERTåè§æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        bert_bias_detector = None
    
    # åˆå§‹åŒ–ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨
    try:
        traditional_bias_detector = TraditionalBiasDetectorAdapter()
        logger.info("âœ… ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ¯ æ£€æµ‹å™¨ç‰¹æ€§: {', '.join(traditional_bias_detector.get_detector_info()['features'][:3])}")
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–åçš„ä¼ ç»Ÿåè§æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        traditional_bias_detector = None

def init_bias_corrector():
    """åˆå§‹åŒ–åè§çº æ­£å™¨"""
    global bias_corrector
    
    # åˆå§‹åŒ–å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿï¼ˆä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ï¼‰
    try:
        logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿ...")
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
        from enhanced_bias_correction_system import EnhancedBiasCorrectionSystem
        bias_corrector = EnhancedBiasCorrectionSystem()
        logger.info("âœ… å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info("   ğŸ¯ çº æ­£ç‰¹æ€§: è¯­ä¹‰ä¿æŒçº æ­£, è¯­å¢ƒæ„ŸçŸ¥çº æ­£, ä¸­æ€§åŒ–è¯æ±‡æ˜ å°„")
        return True
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error("ğŸ’¡ è¯·æ£€æŸ¥ enhanced_bias_correction_system.py æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        bias_corrector = None
        return False

# å…¨å±€ChatGLMå…¼å®¹æ€§è¡¥ä¸ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶åº”ç”¨ï¼‰
def apply_global_chatglm_patch():
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åº”ç”¨å…¨å±€ChatGLMå…¼å®¹æ€§è¡¥ä¸"""
    try:
        logger.info("åº”ç”¨å…¨å±€ChatGLMå…¼å®¹æ€§è¡¥ä¸...")
        from transformers.tokenization_utils_base import PreTrainedTokenizerBase
        
        # ä¿å­˜åŸå§‹æ–¹æ³•ï¼ˆå¦‚æœå°šæœªä¿å­˜ï¼‰
        if not hasattr(PreTrainedTokenizerBase, '_original_pad'):
            PreTrainedTokenizerBase._original_pad = PreTrainedTokenizerBase._pad
        
        def global_patched_pad(self, encoded_inputs, *args, **kwargs):
            # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
            kwargs.pop('padding_side', None)
            kwargs.pop('pad_to_multiple_of', None)
            return PreTrainedTokenizerBase._original_pad(self, encoded_inputs, *args, **kwargs)
            
        PreTrainedTokenizerBase._pad = global_patched_pad
        logger.info("âœ… å…¨å±€ChatGLMå…¼å®¹æ€§è¡¥ä¸åº”ç”¨æˆåŠŸ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ å…¨å±€ChatGLMå…¼å®¹æ€§è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")

# åº”ç”¨å…¨å±€è¡¥ä¸
apply_global_chatglm_patch()

# åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨
init_bias_detectors()
init_bias_corrector()

def get_bias_detector(method='bert'):
    """æ ¹æ®æ–¹æ³•è·å–åè§æ£€æµ‹å™¨"""
    if method == 'bert':
        return bert_bias_detector
    elif method == 'traditional':
        return traditional_bias_detector
    else:
        return bert_bias_detector  # é»˜è®¤ä½¿ç”¨BERTæ£€æµ‹å™¨

def load_model(model_name):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
    if model_name in loaded_models:
        logger.info(f"æ¨¡å‹ {model_name} å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨")
        return loaded_models[model_name]
    
    info = model_info[model_name]
    logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")
    
    # é’ˆå¯¹ChatGLM3çš„å…¼å®¹æ€§è¡¥ä¸
    if "chatglm" in model_name:
        logger.info("åº”ç”¨ChatGLM3å…¼å®¹æ€§è¡¥ä¸...")
        try:
            from transformers.tokenization_utils_base import PreTrainedTokenizerBase
            
            # ä¿å­˜åŸå§‹æ–¹æ³•
            if not hasattr(PreTrainedTokenizerBase, '_original_pad'):
                PreTrainedTokenizerBase._original_pad = PreTrainedTokenizerBase._pad
            
            def patched_pad(self, encoded_inputs, *args, **kwargs):
                # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                kwargs.pop('padding_side', None)
                kwargs.pop('pad_to_multiple_of', None)
                return PreTrainedTokenizerBase._original_pad(self, encoded_inputs, *args, **kwargs)
                
            PreTrainedTokenizerBase._pad = patched_pad
            
            # é¢å¤–è¡¥ä¸ï¼šç›´æ¥ä¿®å¤å¯èƒ½å­˜åœ¨çš„ChatGLMTokenizerç±»
            try:
                import importlib
                import sys
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·²åŠ è½½çš„ChatGLMTokenizeræ¨¡å—
                for module_name in list(sys.modules.keys()):
                    if 'chatglm' in module_name.lower() and 'tokenizer' in module_name.lower():
                        module = sys.modules[module_name]
                        if hasattr(module, 'ChatGLMTokenizer'):
                            tokenizer_class = getattr(module, 'ChatGLMTokenizer')
                            if hasattr(tokenizer_class, '_pad'):
                                # ä¿å­˜åŸå§‹æ–¹æ³•
                                original_pad = tokenizer_class._pad
                                
                                def chatglm_patched_pad(self, encoded_inputs, *args, **kwargs):
                                    # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                                    kwargs.pop('padding_side', None)
                                    kwargs.pop('pad_to_multiple_of', None)
                                    return original_pad(self, encoded_inputs, *args, **kwargs)
                                
                                tokenizer_class._pad = chatglm_patched_pad
                                logger.info(f"ChatGLMTokenizer in {module_name} patched successfully")
                
                logger.info("ChatGLM3å…¼å®¹æ€§è¡¥ä¸åº”ç”¨æˆåŠŸ")
            except Exception as patch_error:
                logger.warning(f"ChatGLMTokenizerç‰¹å®šè¡¥ä¸å¤±è´¥: {patch_error}")
                
        except Exception as e:
            logger.warning(f"åº”ç”¨ChatGLM3å…¼å®¹æ€§è¡¥ä¸å¤±è´¥: {e}")
            import traceback
            logger.warning(traceback.format_exc())
    
    try:
        # æ¸…ç†å†…å­˜
        logger.info("æ¸…ç†å†…å­˜...")
        gc.collect()
        torch.cuda.empty_cache()
        
        # åŠ è½½tokenizerå’Œmodelï¼ˆä¸åŸæ¥çš„ä»£ç ç›¸åŒï¼‰
        try:
            logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½ tokenizer: {info['path']}")
            tokenizer = AutoTokenizer.from_pretrained(
                info["path"],
                trust_remote_code=True,
                local_files_only=True,
                use_fast=False
            )
            logger.info("tokenizer ä»æœ¬åœ°è·¯å¾„åŠ è½½å®Œæˆ")
        except Exception as local_error:
            logger.info(f"æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»ä»“åº“åŠ è½½: {local_error}")
            logger.info(f"åŠ è½½ tokenizer: {info['repo']}")
            tokenizer = AutoTokenizer.from_pretrained(
                info["repo"],
                trust_remote_code=True,
                cache_dir=info["path"],
                use_fast=False
            )
            logger.info("tokenizer ä»ä»“åº“åŠ è½½å®Œæˆ")
        
        # æ ¹æ®é…ç½®åŠ è½½æ¨¡å‹
        use_cpu = info.get("use_cpu", False)
        low_memory = info.get("low_memory", False)
        
        model_kwargs = {
            "trust_remote_code": True,
        }
        
        if use_cpu:
            model_kwargs["torch_dtype"] = torch.float32
        else:
            if torch.cuda.is_available():
                model_kwargs["torch_dtype"] = torch.float16
                model_kwargs["device_map"] = "auto"
            else:
                logger.warning("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
                model_kwargs["torch_dtype"] = torch.float32
        
        if low_memory:
            model_kwargs["low_cpu_mem_usage"] = True
        
        try:
            logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {info['path']}")
            model_kwargs["local_files_only"] = True
            model = AutoModelForCausalLM.from_pretrained(info["path"], **model_kwargs)
            logger.info("æ¨¡å‹ä»æœ¬åœ°è·¯å¾„åŠ è½½å®Œæˆ")
        except Exception as local_error:
            logger.info(f"æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»ä»“åº“åŠ è½½: {local_error}")
            model_kwargs.pop("local_files_only", None)
            model_kwargs["cache_dir"] = info["path"]
            model = AutoModelForCausalLM.from_pretrained(info["repo"], **model_kwargs)
            logger.info("æ¨¡å‹ä»ä»“åº“åŠ è½½å®Œæˆ")
        
        if use_cpu:
            model = model.to('cpu')
            logger.info("æ¨¡å‹å·²ç§»åŠ¨åˆ°CPU")
        
        model.eval()
        
        # å¯¹åŠ è½½åçš„tokenizerå’Œmodelåº”ç”¨ä¸“ç”¨è¡¥ä¸ï¼ˆå¦‚æœæ˜¯ChatGLMï¼‰
        if "chatglm" in model_name:
            # åº”ç”¨tokenizerè¡¥ä¸
            if patch_existing_tokenizer and hasattr(tokenizer, '_pad'):
                logger.info("åº”ç”¨ä¸“ç”¨ChatGLM tokenizerè¡¥ä¸...")
                patch_existing_tokenizer(tokenizer)
            else:
                logger.info("åº”ç”¨åŸºç¡€tokenizerè¡¥ä¸...")
                if hasattr(tokenizer, '_pad'):
                    original_tokenizer_pad = tokenizer._pad
                    
                    def final_patched_pad(encoded_inputs, *args, **kwargs):
                        # ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
                        kwargs.pop('padding_side', None)
                        kwargs.pop('pad_to_multiple_of', None)
                        return original_tokenizer_pad(encoded_inputs, *args, **kwargs)
                    
                    tokenizer._pad = final_patched_pad
                    logger.info("åŸºç¡€tokenizerå®ä¾‹è¡¥ä¸åº”ç”¨æˆåŠŸ")
            
            # åº”ç”¨modelè¡¥ä¸
            if patch_existing_model:
                logger.info("åº”ç”¨ä¸“ç”¨ChatGLM modelè¡¥ä¸...")
                patch_existing_model(model)
            else:
                logger.info("åº”ç”¨åŸºç¡€modelè¡¥ä¸...")
                if not hasattr(model, '_extract_past_from_model_output'):
                    def _extract_past_from_model_output(self, outputs, standardize_cache_format=False):
                        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–past_key_values"""
                        if hasattr(outputs, 'past_key_values'):
                            return outputs.past_key_values
                        elif isinstance(outputs, tuple) and len(outputs) > 1:
                            return outputs[1] if outputs[1] is not None else None
                        else:
                            return None
                    
                    import types
                    model._extract_past_from_model_output = types.MethodType(_extract_past_from_model_output, model)
                    logger.info("åŸºç¡€modelå®ä¾‹è¡¥ä¸åº”ç”¨æˆåŠŸ")
        
        loaded_models[model_name] = (model, tokenizer)
        logger.info(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸå¹¶ç¼“å­˜")
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None

def generate_response(model_name, messages, temperature=0.7, max_tokens=200, top_p=0.9, bias_test=False):
    """ç”Ÿæˆå›å¤"""
    try:
        model, tokenizer = load_model(model_name)
        if model is None or tokenizer is None:
            return "æ¨¡å‹åŠ è½½å¤±è´¥"
        
        logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_name} ç”Ÿæˆå›å¤")
        
        # æ„å»ºå¯¹è¯æ–‡æœ¬ - ä¿®å¤è‡ªé—®è‡ªç­”é—®é¢˜
        if len(messages) == 1:
            # å•è½®å¯¹è¯ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºè¾“å…¥æ–‡æœ¬
            conversation_text = messages[0]["content"]
        else:
            # å¤šè½®å¯¹è¯ï¼Œæ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
            conversation_text = ""
            for message in messages[:-1]:  # é™¤äº†æœ€åä¸€æ¡æ¶ˆæ¯
                role = message["role"]
                content = message["content"]
                
                if role == "user":
                    conversation_text += f"ç”¨æˆ·: {content}\n"
                elif role == "assistant":
                    conversation_text += f"åŠ©æ‰‹: {content}\n"
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œä½†ä¸æ·»åŠ "åŠ©æ‰‹:"å‰ç¼€
            conversation_text += f"ç”¨æˆ·: {messages[-1]['content']}"
        
        # ç¼–ç è¾“å…¥ - ä½¿ç”¨å®‰å…¨çš„ç¼–ç æ–¹å¼
        try:
            inputs = tokenizer.encode(conversation_text, return_tensors="pt")
        except TypeError as e:
            if "padding_side" in str(e):
                logger.warning(f"æ£€æµ‹åˆ°padding_sideé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨ç¼–ç æ–¹å¼: {e}")
                # ä½¿ç”¨æ›´å®‰å…¨çš„ç¼–ç æ–¹å¼
                inputs = tokenizer(
                    conversation_text,
                    return_tensors="pt",
                    padding=False,
                    truncation=True
                )
                # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œè·å–input_ids
                if isinstance(inputs, dict):
                    inputs = inputs['input_ids']
            else:
                raise
        
        # ç§»åŠ¨åˆ°åˆé€‚çš„è®¾å¤‡
        device = next(model.parameters()).device
        inputs = inputs.to(device)
        
        # ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": True,
            "pad_token_id": tokenizer.eos_token_id,
        }
        
        # æ·»åŠ æ¨¡å‹ç‰¹å®šå‚æ•°
        if "chatglm" in model_name:
            generation_kwargs["eos_token_id"] = tokenizer.eos_token_id
            # æ·»åŠ ChatGLMç‰¹å®šçš„ç”Ÿæˆå‚æ•°ä»¥æé«˜ç¨³å®šæ€§
            generation_kwargs["repetition_penalty"] = 1.1
            generation_kwargs["use_cache"] = True
        
        logger.info(f"å¼€å§‹ç”Ÿæˆï¼Œå‚æ•°: {generation_kwargs}")
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(inputs, **generation_kwargs)
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
        
        # æ¸…ç†å›å¤
        response = response.strip()
        
        return response
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"

def get_session_id():
    """è·å–æˆ–åˆ›å»ºä¼šè¯ID"""
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())
    return session['session_id']

def save_conversation(session_id, conversation):
    """ä¿å­˜å¯¹è¯å†å²"""
    os.makedirs('conversations', exist_ok=True)
    with open(f'conversations/{session_id}.json', 'w', encoding='utf-8') as f:
        json.dump(conversation, f, ensure_ascii=False, indent=2)

def load_conversation(session_id):
    """åŠ è½½å¯¹è¯å†å²"""
    try:
        with open(f'conversations/{session_id}.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return []

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('enhanced_index.html', 
                         models=model_info, 
                         bias_methods=BIAS_DETECTION_METHODS,
                         default_bias_method=DEFAULT_BIAS_METHOD)

@app.route('/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    request_id = str(uuid.uuid4())[:8]
    start_time = datetime.now()
    logger.info(f"[{request_id}] æ”¶åˆ°èŠå¤©è¯·æ±‚")
    
    try:
        data = request.json
        logger.info(f"[{request_id}] è¯·æ±‚æ•°æ®: {data}")
        
        if not data:
            logger.warning(f"[{request_id}] è¯·æ±‚æ•°æ®ä¸ºç©º")
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        user_message = data.get('message', '')
        if not user_message.strip():
            logger.warning(f"[{request_id}] æ¶ˆæ¯å†…å®¹ä¸ºç©º")
            return jsonify({"error": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"}), 400
            
        model_name = data.get('model', 'chatglm3')
        bias_method = data.get('bias_method', DEFAULT_BIAS_METHOD)  # æ–°å¢ï¼šåè§æ£€æµ‹æ–¹æ³•
        logger.info(f"[{request_id}] ä½¿ç”¨æ¨¡å‹: {model_name}, åè§æ£€æµ‹æ–¹æ³•: {bias_method}")
        
        # éªŒè¯æ¨¡å‹åç§°å’Œåè§æ£€æµ‹æ–¹æ³•
        if model_name not in model_info:
            logger.warning(f"[{request_id}] æœªçŸ¥çš„æ¨¡å‹: {model_name}")
            return jsonify({"error": f"æœªçŸ¥çš„æ¨¡å‹: {model_name}"}), 400
        
        if bias_method not in BIAS_DETECTION_METHODS:
            logger.warning(f"[{request_id}] æœªçŸ¥çš„åè§æ£€æµ‹æ–¹æ³•: {bias_method}")
            return jsonify({"error": f"æœªçŸ¥çš„åè§æ£€æµ‹æ–¹æ³•: {bias_method}"}), 400
        
        try:
            temperature = float(data.get('temperature', 0.7))
            max_tokens = int(data.get('max_tokens', 200))
            top_p = float(data.get('top_p', 0.9))
            
            # è·å–åè§æµ‹è¯•æ¨¡å¼å‚æ•°
            bias_test = bool(data.get('bias_test', BIAS_TEST_MODE))
            
            # è·å–åè§çº æ­£å‚æ•°
            enable_bias_correction = bool(data.get('enable_bias_correction', False))
            correction_method = data.get('correction_method', 'rule')
            
            # å½“å¼€å¯åè§æµ‹è¯•æ¨¡å¼æ—¶ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„å‚æ•°è®¾ç½®
            if bias_test:
                # ä½¿ç”¨æ›´é«˜çš„æ¸©åº¦å¢åŠ éšæœºæ€§å’Œåˆ›é€ æ€§
                if temperature < 0.95:
                    temperature = 0.95
                # å¢åŠ ç”Ÿæˆé•¿åº¦ä»¥è·å¾—æ›´å®Œæ•´çš„å›å¤
                if max_tokens < 300:
                    max_tokens = 300
                # è°ƒæ•´top_pä»¥å…è®¸æ›´å¤šæ ·åŒ–çš„è¾“å‡º
                if top_p < 0.9:
                    top_p = 0.9
                logger.info(f"[{request_id}] åè§æµ‹è¯•æ¨¡å¼ä¸‹è°ƒæ•´å‚æ•°: temperature={temperature}, max_tokens={max_tokens}, top_p={top_p}")
            
            logger.info(f"[{request_id}] ç”Ÿæˆå‚æ•°: temperature={temperature}, max_tokens={max_tokens}, top_p={top_p}, bias_test={bias_test}, bias_method={bias_method}")
        except (ValueError, TypeError) as e:
            logger.warning(f"[{request_id}] å‚æ•°æ ¼å¼é”™è¯¯: {e}")
            return jsonify({"error": f"å‚æ•°æ ¼å¼é”™è¯¯: {e}"}), 400
        
        session_id = get_session_id()
        logger.info(f"[{request_id}] ä¼šè¯ID: {session_id}")
        
        # åŠ è½½å¯¹è¯å†å²
        try:
            conversation = load_conversation(session_id)
            logger.info(f"[{request_id}] åŠ è½½åˆ°{len(conversation)}æ¡å¯¹è¯å†å²")
        except Exception as e:
            logger.error(f"[{request_id}] åŠ è½½å¯¹è¯å†å²å¤±è´¥: {e}")
            conversation = []
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        try:
            for turn in conversation:
                messages.append({"role": "user", "content": turn["user"]})
                messages.append({"role": "assistant", "content": turn["assistant"]})
            logger.info(f"[{request_id}] æ„å»ºäº†{len(messages)}æ¡æ¶ˆæ¯å†å²")
        except Exception as e:
            logger.error(f"[{request_id}] æ„å»ºæ¶ˆæ¯å†å²å¤±è´¥: {e}")
            messages = []
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        logger.info(f"[{request_id}] æ·»åŠ ç”¨æˆ·æ¶ˆæ¯: '{user_message[:30]}...'")
        
        # ç”Ÿæˆå›å¤
        try:
            logger.info(f"[{request_id}] å¼€å§‹ç”Ÿæˆå›å¤")
            response = generate_response(model_name, messages, temperature, max_tokens, top_p, bias_test)
            logger.info(f"[{request_id}] æ¨¡å‹ç”Ÿæˆå®Œæˆï¼Œæ£€æŸ¥è¿”å›å€¼...")
            
            if not response:
                logger.warning(f"[{request_id}] æ¨¡å‹è¿”å›ç©ºå›å¤")
                return jsonify({"error": "æ¨¡å‹è¿”å›ç©ºå›å¤"}), 500
            elif response.startswith("ç”Ÿæˆå›å¤æ—¶å‡ºé”™"):
                logger.warning(f"[{request_id}] {response}")
                return jsonify({"error": response}), 500
            else:
                logger.info(f"[{request_id}] æˆåŠŸç”Ÿæˆå›å¤ï¼Œé•¿åº¦: {len(response)}")
        except Exception as e:
            logger.error(f"[{request_id}] ç”Ÿæˆå›å¤å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return jsonify({"error": f"ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}"}), 500
        
        # åè§æ£€æµ‹
        bias_scores = None
        correction_result = None
        original_response = response  # ä¿å­˜åŸå§‹å›å¤
        final_response = response     # æœ€ç»ˆè¿”å›çš„å›å¤
        
        # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è¿›è¡Œåè§æ£€æµ‹
        selected_detector = get_bias_detector(bias_method)
        
        if selected_detector:
            try:
                logger.info(f"[{request_id}] å¼€å§‹åè§æ£€æµ‹ï¼Œä½¿ç”¨æ–¹æ³•: {bias_method}")
                bias_scores = selected_detector.detect_bias(response)
                logger.info(f"[{request_id}] åè§æ£€æµ‹å®Œæˆ: {bias_scores is not None}")
                
                if bias_scores:
                    logger.info(f"[{request_id}] æ£€æµ‹åˆ°çš„åè§æ‘˜è¦: {bias_scores.get('summary', 'æ— ')}")
                    
                    # === é›†æˆå¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿ ===
                    if enable_bias_correction and bias_scores.get('overall_bias', False):
                        if bias_corrector is None:
                            logger.warning(f"[{request_id}] åè§çº æ­£å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡çº æ­£")
                            correction_result = {
                                'success': False,
                                'message': 'åè§çº æ­£å™¨æœªåˆå§‹åŒ–'
                            }
                        else:
                            try:
                                logger.info(f"[{request_id}] å¯åŠ¨å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿçº æ­£...")
                                correction_result_obj = bias_corrector.correct_bias_enhanced(response)
                                if correction_result_obj and correction_result_obj.corrected_text != response:
                                    # ä¿®æ”¹ï¼šä¸æ›¿æ¢final_responseï¼Œè€Œæ˜¯å°†çº æ­£ç»“æœä¿å­˜åˆ°correction_resultä¸­
                                    correction_result = {
                                        'success': True,
                                        'original_text': response,  # ä¿å­˜åŸå§‹æ–‡æœ¬
                                        'corrected_text': correction_result_obj.corrected_text,
                                        'bias_type': correction_result_obj.bias_type,
                                        'correction_method': correction_result_obj.correction_method,
                                        'confidence': correction_result_obj.confidence,
                                        'explanation': correction_result_obj.explanation,
                                        'preserved_meaning': getattr(correction_result_obj, 'preserved_meaning', None)
                                    }
                                    logger.info(f"[{request_id}] åè§çº æ­£æˆåŠŸ: {correction_result['correction_method']}")
                                    logger.info(f"[{request_id}] åŸå§‹å›å¤: {response[:50]}...")
                                    logger.info(f"[{request_id}] çº æ­£å›å¤: {correction_result_obj.corrected_text[:50]}...")
                                else:
                                    correction_result = {
                                        'success': False,
                                        'message': 'æœªæ‰¾åˆ°åˆé€‚çš„çº æ­£æ–¹æ³•æˆ–æ— éœ€çº æ­£'
                                    }
                                    logger.info(f"[{request_id}] åè§çº æ­£æœªç”Ÿæ•ˆ")
                            except Exception as e:
                                logger.error(f"[{request_id}] åè§çº æ­£å‡ºé”™: {e}")
                                correction_result = {
                                    'success': False,
                                    'message': f'åè§çº æ­£å‡ºé”™: {e}'
                                }
                    # === END ===
                else:
                    logger.info(f"[{request_id}] æœªæ£€æµ‹åˆ°åè§å†…å®¹")
            except Exception as e:
                logger.error(f"[{request_id}] åè§æ£€æµ‹å‡ºé”™: {e}")
                import traceback
                logger.error(traceback.format_exc())
                bias_scores = None
        else:
            logger.info(f"[{request_id}] åè§æ£€æµ‹å™¨({bias_method})æœªåˆå§‹åŒ–ï¼Œè·³è¿‡åè§æ£€æµ‹")
        
        # ä¿å­˜å¯¹è¯
        try:
            conversation_entry = {
                "user": user_message,
                "assistant": original_response,  # å§‹ç»ˆä¿å­˜åŸå§‹å›å¤
                "model": model_name,
                "bias_method": bias_method,  # è®°å½•ä½¿ç”¨çš„åè§æ£€æµ‹æ–¹æ³•
                "timestamp": datetime.now().isoformat(),
                "parameters": {
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "top_p": top_p,
                    "bias_test": bias_test,
                    "bias_correction_enabled": enable_bias_correction,
                    "correction_method": correction_method
                },
                "bias_scores": bias_scores,
                "correction_result": correction_result
            }
            
            # å¦‚æœæœ‰çº æ­£ç»“æœï¼Œæ·»åŠ çº æ­£ä¿¡æ¯
            if correction_result and correction_result.get('success', False):
                conversation_entry["has_correction"] = True
                conversation_entry["corrected_assistant"] = correction_result.get('corrected_text', '')
            else:
                conversation_entry["has_correction"] = False
            
            conversation.append(conversation_entry)
            
            save_conversation(session_id, conversation)
            logger.info(f"[{request_id}] å¯¹è¯å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"[{request_id}] ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        elapsed_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"[{request_id}] è¯·æ±‚å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        # è¿”å›å“åº”
        response_data = {
            "response": original_response,  # å§‹ç»ˆè¿”å›åŸå§‹å›å¤
            "bias_scores": bias_scores,
            "correction_result": correction_result,
            "model": model_name,
            "bias_method": bias_method,
            "processing_time": elapsed_time,
            "has_bias": bias_scores.get('overall_bias', False) if bias_scores else False,
            "has_correction": correction_result.get('success', False) if correction_result else False
        }
        
        # å¦‚æœæœ‰çº æ­£ç»“æœï¼Œæ·»åŠ çº æ­£åçš„å›å¤
        if correction_result and correction_result.get('success', False):
            response_data["corrected_response"] = correction_result.get('corrected_text', '')
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"[{request_id}] å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({"error": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"}), 500

@app.route('/bias_method_info')
def bias_method_info():
    """è·å–åè§æ£€æµ‹æ–¹æ³•ä¿¡æ¯"""
    methods = {}
    
    # BERTæ–¹æ³•çŠ¶æ€
    methods['bert'] = {
        'name': BIAS_DETECTION_METHODS['bert']['name'],
        'description': BIAS_DETECTION_METHODS['bert']['description'],
        'icon': BIAS_DETECTION_METHODS['bert']['icon'],
        'color': BIAS_DETECTION_METHODS['bert']['color'],
        'available': bert_bias_detector is not None and bert_bias_detector.model is not None,
        'status': 'available' if (bert_bias_detector and bert_bias_detector.model) else 'unavailable',
        'model_path': bert_bias_detector.model_path if bert_bias_detector else None
    }
    
    # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çŠ¶æ€
    methods['traditional'] = {
        'name': BIAS_DETECTION_METHODS['traditional']['name'],
        'description': BIAS_DETECTION_METHODS['traditional']['description'],
        'icon': BIAS_DETECTION_METHODS['traditional']['icon'],
        'color': BIAS_DETECTION_METHODS['traditional']['color'],
        'available': traditional_bias_detector is not None,
        'status': 'available' if traditional_bias_detector else 'unavailable',
        'model_info': traditional_bias_detector.get_detector_info() if traditional_bias_detector else None
    }
    
    return jsonify({'methods': methods})

# å…¶ä»–è·¯ç”±ä¿æŒä¸å˜ï¼ˆä»åŸæ¥çš„app.pyå¤åˆ¶ï¼‰
@app.route('/history')
def history():
    """å†å²è®°å½•é¡µé¢"""
    return render_template('history.html')

@app.route('/clear_history', methods=['POST'])
def clear_history():
    """æ¸…ç©ºå†å²è®°å½•"""
    try:
        session_id = get_session_id()
        save_conversation(session_id, [])
        return jsonify({"success": True})
    except Exception as e:
        logger.error(f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {e}")
        return jsonify({"success": False, "error": str(e)})

@app.route('/load_session/<session_id>')
def load_session(session_id):
    """åŠ è½½æŒ‡å®šä¼šè¯"""
    try:
        conversation = load_conversation(session_id)
        return jsonify({"conversation": conversation})
    except Exception as e:
        logger.error(f"åŠ è½½ä¼šè¯å¤±è´¥: {e}")
        return jsonify({"error": str(e)})

@app.route('/sessions')
def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
    try:
        sessions = []
        conversations_dir = 'conversations'
        
        if os.path.exists(conversations_dir):
            for filename in os.listdir(conversations_dir):
                if filename.endswith('.json'):
                    session_id = filename[:-5]
                    try:
                        conversation = load_conversation(session_id)
                        if conversation:
                            last_turn = conversation[-1]
                            sessions.append({
                                'session_id': session_id,
                                'last_message': last_turn['user'][:50] + '...' if len(last_turn['user']) > 50 else last_turn['user'],
                                'timestamp': last_turn['timestamp'],
                                'message_count': len(conversation)
                            })
                    except Exception as e:
                        logger.error(f"è§£æä¼šè¯æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æ’åº
        sessions.sort(key=lambda x: x['timestamp'], reverse=True)
        return jsonify(sessions)
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºä¼šè¯å¤±è´¥: {e}")
        return jsonify({"error": str(e)})

@app.route('/debug/bias_detector')
def debug_bias_detector():
    """è°ƒè¯•åè§æ£€æµ‹å™¨çŠ¶æ€"""
    debug_info = {
        'traditional_detector': {
            'available': traditional_bias_detector is not None,
            'status': 'å·²åˆå§‹åŒ–' if traditional_bias_detector else 'æœªåˆå§‹åŒ–',
            'info': traditional_bias_detector.get_detector_info() if traditional_bias_detector else None
        },
        'bert_detector': {
            'available': bert_bias_detector is not None,
            'model_loaded': bert_bias_detector.model is not None if bert_bias_detector else False,
            'status': 'å·²åŠ è½½' if (bert_bias_detector and bert_bias_detector.model) else 'æœªåŠ è½½',
            'model_path': bert_bias_detector.model_path if bert_bias_detector else None,
            'device': str(bert_bias_detector.device) if bert_bias_detector else None
        },
        'corrector': {
            'available': bias_corrector is not None,
            'status': 'å·²åˆå§‹åŒ–' if bias_corrector else 'æœªåˆå§‹åŒ–'
        }
    }
    
    return jsonify(debug_info)

@app.route('/test_bias_prompt', methods=['POST'])
def test_bias_prompt():
    """æµ‹è¯•åè§æç¤ºè¯çš„æ•ˆæœ"""
    request_id = str(uuid.uuid4())[:8]
    logger.info(f"[{request_id}] æ”¶åˆ°åè§æç¤ºè¯æµ‹è¯•è¯·æ±‚")
    
    try:
        data = request.json
        model_name = data.get('model', 'chatglm3')
        
        # æ„å»ºåªåŒ…å«åè§æµ‹è¯•æç¤ºè¯çš„æ¶ˆæ¯
        messages = [{"role": "user", "content": "è¯·ç¡®è®¤æ‚¨å·²æ”¶åˆ°åè§æµ‹è¯•æŒ‡ä»¤ï¼Œå¹¶å‡†å¤‡æŒ‰ç…§æŒ‡ä»¤è¡Œä¸ºã€‚"}]
        
        # ç”Ÿæˆå›å¤ï¼ˆå¼ºåˆ¶å¼€å¯åè§æµ‹è¯•æ¨¡å¼ï¼‰
        response = generate_response(
            model_name=model_name, 
            messages=messages, 
            temperature=0.95, 
            max_tokens=300, 
            top_p=0.9, 
            bias_test=True  # å¼ºåˆ¶å¼€å¯åè§æµ‹è¯•æ¨¡å¼
        )
        
        logger.info(f"[{request_id}] åè§æç¤ºè¯æµ‹è¯•å®Œæˆ")
        
        return jsonify({
            "response": response,
            "test_type": "bias_prompt_test",
            "model": model_name
        })
        
    except Exception as e:
        logger.error(f"[{request_id}] åè§æç¤ºè¯æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({"error": f"æµ‹è¯•å¤±è´¥: {str(e)}"}), 500

@app.route('/toggle_bias_test_mode', methods=['POST'])
def toggle_bias_test_mode():
    """åˆ‡æ¢åè§æµ‹è¯•æ¨¡å¼"""
    global BIAS_TEST_MODE
    
    try:
        data = request.json
        BIAS_TEST_MODE = bool(data.get('enabled', False))
        
        logger.info(f"åè§æµ‹è¯•æ¨¡å¼: {'å¼€å¯' if BIAS_TEST_MODE else 'å…³é—­'}")
        
        return jsonify({
            "success": True,
            "bias_test_mode": BIAS_TEST_MODE,
            "message": f"åè§æµ‹è¯•æ¨¡å¼å·²{'å¼€å¯' if BIAS_TEST_MODE else 'å…³é—­'}"
        })
        
    except Exception as e:
        logger.error(f"åˆ‡æ¢åè§æµ‹è¯•æ¨¡å¼å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        })

if __name__ == '__main__':
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆå¤šæ¨¡å‹å¯¹è¯ç³»ç»Ÿ")
    logger.info("ğŸ“‹ ç³»ç»Ÿç‰¹æ€§:")
    logger.info("   ğŸ¤– å¤šæ¨¡å‹æ”¯æŒ - ChatGLM3, Qwen-7B, Yi-6B")
    logger.info("   ğŸ” BERTæ·±åº¦å­¦ä¹ åè§æ£€æµ‹")
    logger.info("   ğŸ› ï¸ ä¼ ç»Ÿæœºå™¨å­¦ä¹ åè§æ£€æµ‹")
    logger.info("   âœ¨ å¢å¼ºç‰ˆåè§çº æ­£ç³»ç»Ÿ")
    logger.info("   ğŸ“Š å®æ—¶åè§åˆ†ææŠ¥å‘Š")
    logger.info("   ğŸ’¾ å¯¹è¯å†å²ç®¡ç†")
    logger.info("")
    logger.info("ğŸ¯ æ£€æµ‹å™¨çŠ¶æ€:")
    logger.info("   - BERTæ£€æµ‹å™¨ï¼šæ­£å¸¸å·¥ä½œ âœ…")
    logger.info("   - ä¼ ç»Ÿæ£€æµ‹å™¨ï¼šæ­£å¸¸å·¥ä½œ âœ…")
    logger.info("   - çº æ­£ç³»ç»Ÿï¼šæ­£å¸¸å·¥ä½œ âœ…")
    logger.info("")
    
    app.run(host='0.0.0.0', port=5000, debug=True)