#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºCOLDatasetçš„BERTåè§æ£€æµ‹è®­ç»ƒè„šæœ¬
é€‚é…COLDatasetæ•°æ®æ ¼å¼ï¼Œè®­ç»ƒä¸­æ–‡åè§æ£€æµ‹æ¨¡å‹
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EvalPrediction
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class COLDatasetBiasDataset(Dataset):
    """COLDatasetåè§æ£€æµ‹æ•°æ®é›†"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class COLDatasetBertTrainer:
    """åŸºäºCOLDatasetçš„BERTè®­ç»ƒå™¨"""
    
    def __init__(self, output_dir='./coldataset_bias_bert_model'):
        self.output_dir = output_dir
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.class_weights = None
        
        os.makedirs(output_dir, exist_ok=True)
        self.setup_environment()
        
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒé…ç½®"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ...")
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        os.environ['TRANSFORMERS_CACHE'] = './cache'
        os.environ['HF_HOME'] = './cache'
        os.makedirs('./cache', exist_ok=True)
        
    def load_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        
        try:
            # ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„bert-base-chinese
            self.tokenizer = AutoTokenizer.from_pretrained(
                'bert-base-chinese',
                cache_dir='./cache',
                local_files_only=True,
                trust_remote_code=True
            )
            
            self.model = AutoModelForSequenceClassification.from_pretrained(
                'bert-base-chinese',
                num_labels=2,
                cache_dir='./cache',
                local_files_only=True,
                trust_remote_code=True
            )
            
            self.model.to(self.device)
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   æ¨¡å‹: bert-base-chinese")
            print(f"   è®¾å¤‡: {self.device}")
            print(f"   å‚æ•°é‡: {self.model.num_parameters():,}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def analyze_data_distribution(self, df, dataset_name):
        """åˆ†æCOLDatasetæ•°æ®åˆ†å¸ƒ"""
        print(f"\nğŸ“Š {dataset_name}æ•°æ®åˆ†å¸ƒåˆ†æ:")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        label_dist = df['label'].value_counts().sort_index()
        print("æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in label_dist.items():
            percentage = count / len(df) * 100
            label_name = "safe" if label == 0 else "offensive"
            print(f"  {label} ({label_name}): {count:,} ({percentage:.1f}%)")
        
        # ä¸»é¢˜åˆ†å¸ƒ
        if 'topic' in df.columns:
            topic_dist = df['topic'].value_counts()
            print("ä¸»é¢˜åˆ†å¸ƒ:")
            for topic, count in topic_dist.items():
                percentage = count / len(df) * 100
                print(f"  {topic}: {count:,} ({percentage:.1f}%)")
        
        # ç»†ç²’åº¦æ ‡ç­¾åˆ†å¸ƒï¼ˆä»…æµ‹è¯•é›†ï¼‰
        if 'fine-grained-label' in df.columns:
            fine_dist = df['fine-grained-label'].value_counts()
            print("ç»†ç²’åº¦æ ‡ç­¾åˆ†å¸ƒ:")
            for fine_label, count in fine_dist.items():
                percentage = count / len(df) * 100
                print(f"  {fine_label}: {count:,} ({percentage:.1f}%)")
        
        return label_dist
    
    def load_coldataset(self):
        """åŠ è½½COLDatasetæ•°æ®"""
        print("\nğŸ“Š åŠ è½½COLDatasetæ•°æ®...")
        
        # åŠ è½½æ•°æ®
        train_df = pd.read_csv('COLDataset-main/COLDataset/train.csv')
        dev_df = pd.read_csv('COLDataset-main/COLDataset/dev.csv')
        test_df = pd.read_csv('COLDataset-main/COLDataset/test.csv')
        
        # æ•°æ®æ¸…æ´—
        for df_name, df in [('è®­ç»ƒé›†', train_df), ('å¼€å‘é›†', dev_df), ('æµ‹è¯•é›†', test_df)]:
            before_len = len(df)
            df.dropna(subset=['TEXT', 'label'], inplace=True)
            after_len = len(df)
            if before_len != after_len:
                print(f"{df_name}: æ¸…ç† {before_len - after_len} ä¸ªç¼ºå¤±æ ·æœ¬")
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›†{len(train_df)}ï¼Œå¼€å‘é›†{len(dev_df)}ï¼Œæµ‹è¯•é›†{len(test_df)}")
        
        # åˆ†æåŸå§‹æ•°æ®åˆ†å¸ƒ
        for df_name, df in [('è®­ç»ƒé›†', train_df), ('å¼€å‘é›†', dev_df), ('æµ‹è¯•é›†', test_df)]:
            self.analyze_data_distribution(df, df_name)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        self.class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_df['label']),
            y=train_df['label']
        )
        self.class_weights = torch.tensor(self.class_weights, dtype=torch.float).to(self.device)
        
        print(f"\nâš–ï¸ ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆ:")
        print(f"  safe (0): {self.class_weights[0]:.3f}")
        print(f"  offensive (1): {self.class_weights[1]:.3f}")
        
        return train_df, dev_df, test_df
    
    def create_datasets(self, train_df, dev_df, test_df):
        """åˆ›å»ºæ•°æ®é›†"""
        print("ğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
        
        train_dataset = COLDatasetBiasDataset(
            texts=train_df['TEXT'].astype(str).tolist(),
            labels=train_df['label'].tolist(),
            tokenizer=self.tokenizer
        )
        
        dev_dataset = COLDatasetBiasDataset(
            texts=dev_df['TEXT'].astype(str).tolist(),
            labels=dev_df['label'].tolist(),
            tokenizer=self.tokenizer
        )
        
        test_dataset = COLDatasetBiasDataset(
            texts=test_df['TEXT'].astype(str).tolist(),
            labels=test_df['label'].tolist(),
            tokenizer=self.tokenizer
        )
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   å¼€å‘é›†: {len(dev_dataset)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_dataset, dev_dataset, test_dataset
    
    def compute_metrics(self, eval_pred: EvalPrediction):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted', zero_division=0
        )
        
        # è®¡ç®—åè§æ£€æµ‹ä¸“é¡¹æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(
            labels, predictions, average=None, zero_division=0
        )
        
        # safeç±»æŒ‡æ ‡
        safe_recall = recall_per_class[0] if len(recall_per_class) > 0 else 0
        safe_precision = precision_per_class[0] if len(precision_per_class) > 0 else 0
        safe_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0
        
        # offensiveç±»æŒ‡æ ‡
        offensive_recall = recall_per_class[1] if len(recall_per_class) > 1 else 0
        offensive_precision = precision_per_class[1] if len(precision_per_class) > 1 else 0
        offensive_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'safe_recall': safe_recall,
            'safe_precision': safe_precision,
            'safe_f1': safe_f1,
            'offensive_recall': offensive_recall,
            'offensive_precision': offensive_precision,
            'offensive_f1': offensive_f1,
        }
    
    def create_trainer(self, train_dataset, dev_dataset):
        """åˆ›å»ºè®­ç»ƒå™¨"""
        print("ğŸ¯ åˆ›å»ºCOLDatasetè®­ç»ƒå™¨...")
        
        class WeightedTrainer(Trainer):
            def __init__(self, class_weights, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.class_weights = class_weights
                
            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
                labels = inputs.get("labels")
                outputs = model(**inputs)
                logits = outputs.get('logits')
                
                # ä½¿ç”¨åŠ æƒäº¤å‰ç†µæŸå¤±å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
                loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
                loss = loss_fct(logits, labels)
                
                # æ·»åŠ L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
                l2_lambda = 0.001
                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
                loss = loss + l2_lambda * l2_norm
                
                return (loss, outputs) if return_outputs else loss
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=32,
            warmup_steps=500,
            weight_decay=0.01,
            learning_rate=2e-5,
            eval_strategy="steps",
            eval_steps=200,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="offensive_f1",  # ä¼˜åŒ–offensiveç±»F1
            greater_is_better=True,
            logging_steps=50,
            fp16=torch.cuda.is_available(),
            dataloader_num_workers=2,
            seed=42,
            report_to=None,
            gradient_accumulation_steps=2,
        )
        
        trainer = WeightedTrainer(
            class_weights=self.class_weights,
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=dev_dataset,
            compute_metrics=self.compute_metrics,
        )
        
        return trainer
    
    def train_and_evaluate(self, trainer, test_dataset):
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒ
        train_result = trainer.train()
        
        # ä¿å­˜æ¨¡å‹å’Œtokenizer
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)
        
        print("âœ… è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è¯„ä¼°...")
        
        # è¯„ä¼°æµ‹è¯•é›†
        predictions = trainer.predict(test_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
        
        # æ¯ç±»æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # æ„å»ºè¯¦ç»†ç»“æœ
        results = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'safe_precision': float(precision_per_class[0]) if len(precision_per_class) > 0 else 0,
            'safe_recall': float(recall_per_class[0]) if len(recall_per_class) > 0 else 0,
            'safe_f1': float(f1_per_class[0]) if len(f1_per_class) > 0 else 0,
            'offensive_precision': float(precision_per_class[1]) if len(precision_per_class) > 1 else 0,
            'offensive_recall': float(recall_per_class[1]) if len(recall_per_class) > 1 else 0,
            'offensive_f1': float(f1_per_class[1]) if len(f1_per_class) > 1 else 0,
            'confusion_matrix': cm.tolist(),
            'training_time': str(datetime.now()),
        }
        
        # ä¿å­˜ç»“æœ
        with open(f'{self.output_dir}/training_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # è¯¦ç»†ç»“æœè¾“å‡º
        print("\n" + "="*60)
        print("ğŸ¯ COLDatasetæ¨¡å‹è®­ç»ƒå®Œæˆ! è¯¦ç»†ç»“æœ:")
        print("="*60)
        
        print(f"æ€»ä½“æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  åŠ æƒç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  åŠ æƒå¬å›ç‡: {recall:.4f}")
        print(f"  åŠ æƒF1: {f1:.4f}")
        
        print(f"\nå„ç±»åˆ«è¯¦ç»†æ€§èƒ½:")
        print(f"Safeç±»æ£€æµ‹:")
        print(f"  ç²¾ç¡®ç‡: {results['safe_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['safe_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['safe_f1']:.4f}")
        
        print(f"Offensiveç±»æ£€æµ‹:")
        print(f"  ç²¾ç¡®ç‡: {results['offensive_precision']:.4f}")
        print(f"  å¬å›ç‡: {results['offensive_recall']:.4f}")
        print(f"  F1åˆ†æ•°: {results['offensive_f1']:.4f}")
        
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"  é¢„æµ‹\\çœŸå®    Safe    Offensive")
        print(f"  Safe       {cm[0,0]:4d}    {cm[0,1]:4d}")
        print(f"  Offensive  {cm[1,0]:4d}    {cm[1,1]:4d}")
        
        # æ€§èƒ½è¯„ä»·
        offensive_f1 = results['offensive_f1']
        print(f"\nğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
        if offensive_f1 > 0.8:
            print("âœ… Offensiveæ£€æµ‹F1ä¼˜ç§€! (>0.8)")
        elif offensive_f1 > 0.7:
            print("âœ… Offensiveæ£€æµ‹F1è‰¯å¥½! (>0.7)")
        elif offensive_f1 > 0.6:
            print("âš ï¸ Offensiveæ£€æµ‹F1ä¸€èˆ¬ (>0.6)")
        else:
            print("âŒ Offensiveæ£€æµ‹F1è¾ƒä½ (<0.6)")
        
        print(f"ğŸ“ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        
        return results
    
    def run_training(self):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹COLDatasetåè§æ£€æµ‹æ¨¡å‹è®­ç»ƒ...")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # 1. åŠ è½½æ¨¡å‹
        if not self.load_model():
            return False
        
        # 2. åŠ è½½æ•°æ®
        train_df, dev_df, test_df = self.load_coldataset()
        
        # 3. åˆ›å»ºæ•°æ®é›†
        train_dataset, dev_dataset, test_dataset = self.create_datasets(train_df, dev_df, test_df)
        
        # 4. åˆ›å»ºè®­ç»ƒå™¨
        trainer = self.create_trainer(train_dataset, dev_dataset)
        
        # 5. è®­ç»ƒå’Œè¯„ä¼°
        results = self.train_and_evaluate(trainer, test_dataset)
        
        print("ğŸ‰ COLDatasetæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return True

def main():
    """ä¸»å‡½æ•°"""
    trainer = COLDatasetBertTrainer()
    success = trainer.run_training()
    
    if success:
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥!")

if __name__ == "__main__":
    main() 