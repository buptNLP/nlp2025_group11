import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from traditional_bias_detector import TraditionalBiasDetector
from coldataset_bias_trainer import COLDatasetBertTrainer
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import jieba
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

class BertBiasPredictor:
    """BERTåè§æ£€æµ‹é¢„æµ‹å™¨"""
    
    def __init__(self, model_path='./coldataset_bias_bert_model'):
        self.model_path = model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = None
        self.model = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„BERTæ¨¡å‹"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            print("âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def detect_bias(self, text):
        """æ£€æµ‹æ–‡æœ¬åè§"""
        if not self.model or not self.tokenizer:
            return {'bias_types': [], 'confidence': 0.0}
        
        # ç¼–ç æ–‡æœ¬
        inputs = self.tokenizer(
            text,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors='pt'
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            prediction = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][prediction].item()
        
        # è¿”å›ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´ï¼‰
        if prediction == 1:  # æœ‰åè§
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æœ‰æ›´å¤æ‚çš„é€»è¾‘æ¥ç¡®å®šåè§ç±»å‹
            return {
                'bias_types': ['general'],  # ç®€åŒ–ä¸ºé€šç”¨åè§
                'confidence': confidence
            }
        else:
            return {
                'bias_types': [],
                'confidence': confidence
            }

def load_test_data():
    """åŠ è½½COLDatasetæµ‹è¯•é›†ï¼ˆCSVæ ¼å¼ï¼‰"""
    df = pd.read_csv('COLDataset-main/COLDataset/test.csv', encoding='utf-8')
    # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼šTEXTä¸ºæ–‡æœ¬å†…å®¹ï¼Œtopicä¸ºåè§ç±»å‹ï¼Œlabelä¸ºæ˜¯å¦æœ‰åè§
    test_data = []
    for _, row in df.iterrows():
        test_data.append({
            'text': row['TEXT'],
            'bias_type': row['topic'],  # race, gender, regionç­‰
            'has_bias': row['label']    # 1è¡¨ç¤ºæœ‰åè§ï¼Œ0è¡¨ç¤ºæ— åè§
        })
    return test_data

def evaluate_model(model, test_data, model_type):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - ç®€åŒ–ç‰ˆï¼šåªè¯„ä¼°åè§è¯†åˆ«èƒ½åŠ›"""
    results = {
        'pred': [],
        'true': []
    }
    
    for item in tqdm(test_data, desc=f'è¯„ä¼°{model_type}æ¨¡å‹'):
        text = item['text']
        has_bias = item['has_bias']
        
        # çœŸå®æ ‡ç­¾ï¼š1è¡¨ç¤ºæœ‰åè§ï¼Œ0è¡¨ç¤ºæ— åè§
        true_label = has_bias
        
        # è·å–æ¨¡å‹é¢„æµ‹ç»“æœ
        pred = model.detect_bias(text)
        
        if model_type == 'BERT':
            # BERTæ¨¡å‹è¿”å›é€šç”¨åè§æ£€æµ‹ç»“æœ
            detected_bias_types = pred['bias_types'] if pred['bias_types'] else []
            # é¢„æµ‹æ ‡ç­¾ï¼šæ£€æµ‹åˆ°ä»»ä½•åè§ç±»å‹å°±ä¸º1
            pred_label = 1 if len(detected_bias_types) > 0 else 0
                
        else:
            # ä¼ ç»Ÿæ¨¡å‹è¿”å›æ ¼å¼ï¼š{'summary': {'is_biased': bool, 'bias_types': list}}
            pred = model.detect_bias(text, threshold_svm=0.5)
            is_biased = pred['summary']['is_biased']
            # é¢„æµ‹æ ‡ç­¾ï¼šæ¨¡å‹åˆ¤å®šä¸ºåè§å°±ä¸º1
            pred_label = 1 if is_biased else 0
        
        results['pred'].append(pred_label)
        results['true'].append(true_label)
    
    return results

def plot_confusion_matrix(results, model_type):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ - ç®€åŒ–ç‰ˆ"""
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    
    cm = confusion_matrix(results['true'], results['pred'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=['éåè§', 'åè§'],
               yticklabels=['éåè§', 'åè§'],
               ax=ax)
    ax.set_title(f'{model_type}æ¨¡å‹åè§æ£€æµ‹æ··æ·†çŸ©é˜µ', fontsize=16)
    ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax.set_ylabel('çœŸå®æ ‡ç­¾')
    
    plt.tight_layout()
    plt.savefig(f'{model_type}_bias_detection_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()

def calculate_metrics(results):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ç®€åŒ–ç‰ˆ"""
    true = np.array(results['true'])
    pred = np.array(results['pred'])
    
    tp = np.sum((true == 1) & (pred == 1))
    fp = np.sum((true == 0) & (pred == 1))
    fn = np.sum((true == 1) & (pred == 0))
    tn = np.sum((true == 0) & (pred == 0))
    
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    return {
        'å‡†ç¡®ç‡': accuracy,
        'ç²¾ç¡®ç‡': precision,
        'å¬å›ç‡': recall,
        'F1åˆ†æ•°': f1,
        'è¯¯åˆ¤ç‡': false_positive_rate,
        'TP': tp,
        'FP': fp,
        'FN': fn,
        'TN': tn
    }

def plot_metrics_comparison(bert_metrics, traditional_metrics):
    """ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾ - ç®€åŒ–ç‰ˆ"""
    metrics = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'è¯¯åˆ¤ç‡']
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    x = np.arange(len(metrics))
    width = 0.35
    
    bert_values = [bert_metrics[m] for m in metrics]
    traditional_values = [traditional_metrics[m] for m in metrics]
    
    ax.bar(x - width/2, bert_values, width, label='BERTæ¨¡å‹')
    ax.bar(x + width/2, traditional_values, width, label='ä¼ ç»Ÿæ¨¡å‹')
    
    ax.set_title('åè§æ£€æµ‹æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics, rotation=45)
    ax.legend()
    ax.set_ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig('bias_detection_metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def main():
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    test_data = load_test_data()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    bert_model = BertBiasPredictor()
    traditional_model = TraditionalBiasDetector()
    
    # è¯„ä¼°BERTæ¨¡å‹
    print("è¯„ä¼°BERTæ¨¡å‹...")
    bert_results = evaluate_model(bert_model, test_data, 'BERT')
    
    # è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹
    print("è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹...")
    traditional_results = evaluate_model(traditional_model, test_data, 'Traditional')
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix(bert_results, 'BERT')
    plot_confusion_matrix(traditional_results, 'Traditional')
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    bert_metrics = calculate_metrics(bert_results)
    traditional_metrics = calculate_metrics(traditional_results)
    
    # ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾
    print("ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾...")
    plot_metrics_comparison(bert_metrics, traditional_metrics)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    print("ä¿å­˜è¯„ä¼°ç»“æœ...")
    results_df = pd.DataFrame({
        'æ¨¡å‹': ['BERT', 'Traditional'],
        'å‡†ç¡®ç‡': [bert_metrics['å‡†ç¡®ç‡'], traditional_metrics['å‡†ç¡®ç‡']],
        'ç²¾ç¡®ç‡': [bert_metrics['ç²¾ç¡®ç‡'], traditional_metrics['ç²¾ç¡®ç‡']],
        'å¬å›ç‡': [bert_metrics['å¬å›ç‡'], traditional_metrics['å¬å›ç‡']],
        'F1åˆ†æ•°': [bert_metrics['F1åˆ†æ•°'], traditional_metrics['F1åˆ†æ•°']],
        'è¯¯åˆ¤ç‡': [bert_metrics['è¯¯åˆ¤ç‡'], traditional_metrics['è¯¯åˆ¤ç‡']],
        'TP': [bert_metrics['TP'], traditional_metrics['TP']],
        'FP': [bert_metrics['FP'], traditional_metrics['FP']],
        'FN': [bert_metrics['FN'], traditional_metrics['FN']],
        'TN': [bert_metrics['TN'], traditional_metrics['TN']]
    })
    
    results_df.to_csv('bias_detection_comparison_results.csv', index=False, encoding='utf-8-sig')
    print("è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° bias_detection_comparison_results.csv")
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“Š åè§æ£€æµ‹è¯„ä¼°ç»“æœæ‘˜è¦:")
    print("="*60)
    print(f"BERTæ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {bert_metrics['å‡†ç¡®ç‡']:.3f}")
    print(f"  ç²¾ç¡®ç‡: {bert_metrics['ç²¾ç¡®ç‡']:.3f}")
    print(f"  å¬å›ç‡: {bert_metrics['å¬å›ç‡']:.3f}")
    print(f"  F1åˆ†æ•°: {bert_metrics['F1åˆ†æ•°']:.3f}")
    print(f"  æ··æ·†çŸ©é˜µ: TP={bert_metrics['TP']}, FP={bert_metrics['FP']}, FN={bert_metrics['FN']}, TN={bert_metrics['TN']}")
    
    print(f"\nä¼ ç»Ÿæ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {traditional_metrics['å‡†ç¡®ç‡']:.3f}")
    print(f"  ç²¾ç¡®ç‡: {traditional_metrics['ç²¾ç¡®ç‡']:.3f}")
    print(f"  å¬å›ç‡: {traditional_metrics['å¬å›ç‡']:.3f}")
    print(f"  F1åˆ†æ•°: {traditional_metrics['F1åˆ†æ•°']:.3f}")
    print(f"  æ··æ·†çŸ©é˜µ: TP={traditional_metrics['TP']}, FP={traditional_metrics['FP']}, FN={traditional_metrics['FN']}, TN={traditional_metrics['TN']}")
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print(f"  BERTæ¨¡å‹æ£€æµ‹åˆ°åè§æ ·æœ¬æ•°: {bert_metrics['TP'] + bert_metrics['FP']}")
    print(f"  ä¼ ç»Ÿæ¨¡å‹æ£€æµ‹åˆ°åè§æ ·æœ¬æ•°: {traditional_metrics['TP'] + traditional_metrics['FP']}")
    print(f"  å®é™…åè§æ ·æœ¬æ€»æ•°: {bert_metrics['TP'] + bert_metrics['FN']}")
    print(f"  å®é™…éåè§æ ·æœ¬æ€»æ•°: {bert_metrics['TN'] + bert_metrics['FP']}")

if __name__ == '__main__':
    main() 